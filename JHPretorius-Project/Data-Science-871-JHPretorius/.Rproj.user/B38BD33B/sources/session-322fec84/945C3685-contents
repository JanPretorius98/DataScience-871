---
title: "Data-Science-871-JHPretorius-Project"
author: "JH Pretorius"
date: "2023-05-24"
output: html_document
css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preface

```{r housekeeping, message=FALSE, warning=FALSE, results=FALSE}
# goal: "Threefold: (1) Transformation and cleaning of data. (2) Exploratory Data Analysis of Dating Site Data. (3) Machine Learning modelling."

# Clear environment
rm(list = ls())
options(scipen = 999)

# Load packages in use
pacman::p_load(dplyr, ggplot2, tidyverse, rsample, caret, glmnet, vip, pdp, stringr, 
               tidytext, emoji, stopwords, ggridges, wordcloud2, ggmap, readxl, maps,
               viridis, eurostat, corrplot, GGally, reshape2, grid, rpart, rpart.plot, 
               randomForest, xgboost)

# Define plot themes and palettes
palette <- c("#1beaa7", "#00d9d3", "#00c2ff", "#00a5ff", "#007bff", "#8c2aef")

th <- theme(legend.position = "right",
            panel.background = element_blank(),
            plot.background = element_rect(fill = "#000123", color = "#000123"),
            panel.grid.major = element_line(color="white", size = 0.1),
            panel.grid.minor = element_line(color="white", size = 0.1),
            axis.title.x=element_text(colour="white", size = 12,
                                      family = "arial", vjust=-2,hjust=0.5, face = "bold"),
            axis.title.y=element_text(colour="white", size = 12,
                                      family = "arial",vjust = 3,hjust=0.5, face = "bold"),
            axis.text.y=element_text(colour ="white", size = 10, 
                                     family = "arial"),
            axis.text.x=element_text(colour="white", size = 10,
                                     family = "arial"),
            plot.margin = unit(c(0.5,0.5,0.5,0.5), "cm"),
            plot.title = element_text(colour="white", size = 16,
                                      family = "arial",hjust=0.5, face = "bold"),
            plot.subtitle = element_text(colour="white", size = 14,
                                         family = "arial"),
            plot.caption = element_text(colour="white", size = 10,
                                        family = "arial"),
            legend.text = element_text(colour="white", size = 12,
                                       family = "arial"),
            legend.title = element_text(colour="white", size = 12,
                                        family = "arial", hjust=3, face = "bold"),
            axis.ticks = element_blank(),
            strip.text = element_text(colour="white", size = 12,
                                      family = "arial", vjust=1,hjust=0.5),
            legend.key = element_rect(fill = "#000123", color = "#000123"),
            legend.background = element_rect(fill = "#000123"))

# Import data
path <- "/Users/janhendrikpretorius/Library/CloudStorage/OneDrive-StellenboschUniversity/Masters-2023/Modules/Data Science/DataScience-871-repo/JHPretorius-Project/Candidate Data Sets/Dating/"
file <- "lovoo_v3_users_api-results.csv"

df <- read_csv(paste0(path, file))

```

# Introduction

The objective of this study is to investigate the critical factors that contribute to an individual's appeal, popularity, and recognition within an online dating platform. The data utilised for this research is sourced from Lovoo, a prominent European dating application, and is accessible via [Kaggle](https://www.kaggle.com/datasets/utkarshx27/lovoo-dating-app-dataset?select=lovoo_v3_users_api-results.csv).

The underlying motivation for this study stems from the desire to comprehend behavioural patterns that transcend the confines of physical attractiveness. The aim is to unveil hidden determinants that may shape interpersonal interactions within a digital dating platform. The behaviour exhibited on these platforms carries significance, even in economic contexts. By deciphering this behavioural paradigm, it can potentially contribute to the development of economic models. These enhanced models can subsequently offer a more profound analytic framework to elucidate overall mate-selection behaviour.

The initial phase of the analysis involves the transformation of raw data into a more interpretable format. This includes the creation of additional variables tailored to augment the predictive capacity of the statistical models employed in subsequent stages. This phase facilitates the exploratory aspect of the research, enabling an in-depth examination of data in search of potential predictor variables. The objective extends beyond understanding the phenomena; the aim is to anticipate which factors instigate an increased number of profile views and, subsequently, the 'likes' received.

The modelling process is a two-step approach. The first stage focuses on identifying variables that may elucidate why individuals view a certain profile. Potential variables include online presence, age, geographical location, and the timing of an individual's online activity. The second stage aims to identify factors that influence the likelihood of a profile receiving 'likes'. These may include the number of pictures on a profile, the characteristics of a profile's biography, languages spoken, profile verification status, and mobile usage.

It is noteworthy that the number of 'likes' received by a profile strongly correlates with the number of views that profile has. Consequently, it may be necessary to use a two-stage model where the first stage models the factors influencing profile views, and the subsequent stage uses the outcomes from the first stage to model the likes received by a profile.

A random forest model is employed in this study due to its robust ability to discern intricate characteristics that influence human behaviour. The model serves as a guide towards achieving a comprehensive understanding of the determinants of attractiveness, popularity, and recognition in the digital dating arena.

# Part 1: Transformation and Cleaning

The dataset in consideration comprises approximately 30 variables, each encapsulating specific attributes pertaining to individual profiles and related demographic information. An excerpt of the dataset is provided subsequently, supplemented by Table 1 which elucidates a selection of significant variables. It's noteworthy to mention that the dataset solely encompasses data of individuals identifying as female. As such, the core objective of this analysis is to discern the determinants influencing the behavioural patterns of individuals displaying interest in females.

#### Head of dataframe

```{r echo=FALSE}
head(df)
```

#### Table 1: Description of variables in data set.

| Variable               | Description                                                                                                                                 |
|---------------------|---------------------------------------------------|
| `genderLooking`        | Preferred gender the subject is looking to engage with. Represented as 'M' for male, 'F' for female, 'both' for male and female, or 'none'. |
| `age`                  | Age of the individual.                                                                                                                      |
| `counts_details`       | How complete the profile is. Proportion of detail in the account. Measured from 0.0-1.0.                                                    |
| `counts_pictures`      | How many pictures does the profile contain.                                                                                                 |
| `counts_profileVisits` | How many times the profile has been viewed.                                                                                                 |
| `counts_kisses`        | Number of 'kisses' or 'likes' received by profile.                                                                                          |
| `flirtInterests_*`     | What the individual is interested in. '\*' represents: 'chat', 'date', 'friends'.                                                           |
| `verified`             | Whether the profile has been verified or not.                                                                                               |
| `lang_count`           | Number of languages spoken by an individual.                                                                                                |
| `lang_*`               | Language spoken by an individual. '\*' represents: 'en' (English), 'de' (German), 'fr' (French), 'it' (Italian), 'es' (Spanish).            |
| `whazzup`              | A phrase that represents the profile's 'bio'.                                                                                               |

The original dataset is already quite useable, but we can produce better models by adding some new variables. The first step is to take a closer look at the language people use in their profiles. I am focusing on two main things here: the words used in the profile descriptions, and the use of emojis. Both of these could give insights into the person's confidence and desirability.

The code chunk below creates two new dummy variables, `has_emoji` and `contains_popular_word`. `has_emoji` attributes a '1' based on whether `wazzup` contains an emoji. `contains_popular_word` attributes a '1' based on whether `wazzup` contains a popular word. The code chunk also outputs which words are the most popular in a word cloud. (The word cloud is a dynamic image that shows the popularity when hovering over a specific word)

```{r words}

# Define stop words for different languages
all_stop_words <- c(stopwords::stopwords("de"), stopwords::stopwords("en"), stopwords::stopwords("fr"))

# Define dummy variable that detects presence of emojis
# Also remove digits from 'whazzup' column
df <- df %>%
  mutate(has_emoji = ifelse(emoji_detect(whazzup), 1, 0),
         whazzup = str_remove_all(whazzup, "[[:digit:]]+"))

# Get the most used words in profile
# First, create a table of words with the corresponding counts_profileVisits
words_visits <- df %>%
  unnest_tokens(word, whazzup) %>%
  select(word, counts_profileVisits)

# Then calculate the mean counts_profileVisits for each word and its count
words <- words_visits %>%
  group_by(word) %>%
  summarise(mean_profileVisits = mean(counts_profileVisits, na.rm = TRUE),
            word_count = n(), 
            .groups = "drop")

# Create word popularity index and determine popular words
words <- words %>%
  mutate(popularity_index = 0.8 * word_count + 0.2 * mean_profileVisits) %>% 
  filter(popularity_index > 200 & word_count > 10, 
         word != "", 
         !is.na(word), 
         !is.na(mean_profileVisits), 
         !is.na(word_count), 
         !is.na(popularity_index)) %>%
  filter(!word %in% all_stop_words)

# Create a single pattern string that matches any word in words$word
words_pattern <- paste(words$word, collapse = "|")

# Add the new variable to df
df <- df %>%
  mutate(contains_popular_word = ifelse(str_detect(whazzup, words_pattern), 1, 0)) %>%
  mutate(
    contains_popular_word = replace_na(contains_popular_word, 0),
    has_emoji = replace_na(has_emoji, 0)
  )

words <- words %>%
  arrange(desc(popularity_index)) %>% 
  select(c(word, popularity_index))

```

#### Figure 1: Word cloud of popular words in bios.

```{r echo=FALSE, fig.align="center"}
# Create Word Cloud
wordcloud2(words, size=1.6, color='random-light', backgroundColor = "#000123")
```

Interestingly, one may see that popular words (those are words that get many profile views and are used frequently), are social media tags. That is, individuals that have their social media details, such as their Instagram handle, Facebook name, and Snapchat handle, on their account tend to get more profile views. As such, I will create another variable, called `has_social` that captures whether a profile contains social media particulars. Due to endogeneity and possible multicollinearity between `contains_popular_word` and `has_social`, only one can be used in modelling. Whichever delivers the most accurate result will then be used.

```{r warning=FALSE}

social <- "instagram|insta|facebook|fb|snapchat|snap"
df <- df %>%
  mutate(whazzup = tolower(whazzup),
         has_social = as.numeric(str_detect(whazzup, social))) %>%
  replace_na(list(has_social = 0))

```

Another operation pertains to the time an individual is online. I add a new dummy variable called `night_owl` to the dataframe based on whether a person was online at night or not. The motivation behind this is that dating apps tend to be more popular in the evenings, than during daytime.

```{r, warning=FALSE}
df <- df %>%
  mutate(night_owl = ifelse(hour(hms(substr(lastOnlineDate, 12, 19))) > 18 | 
                            hour(hms(substr(lastOnlineDate, 12, 19))) < 6, 1, 0))

```

Then, it may be beneficial to categorise profile views and likes as 'low', 'medium', and 'high'. This was done by splitting the variables based on quantiles.

```{r}

# Categorize counts_profileVisits and counts_kisses based on quartiles
df$Profile_Views <- cut(df$counts_profileVisits, 
                                     breaks = quantile(df$counts_profileVisits, probs = 0:4/4, na.rm = TRUE), 
                                     labels = c("Low", "Low Mid", "High Mid", "High"), include.lowest = TRUE)

df$Profile_Likes <- cut(df$counts_kisses, 
                              breaks = quantile(df$counts_kisses, probs = 0:4/4, na.rm = TRUE), 
                              labels = c("Low", "Low Mid", "High Mid", "High"), include.lowest = TRUE)


```


Finally, before kicking off with the exploratory data analysis, it might be fitting to standardise the following three variables: `counts_profileVisits`, `counts_kisses`, and `counts_pictures`. This operation may help to more clearly visualise how some variables influence each other.

```{r}

df$counts_profileVisitsStd <- scale(df$counts_profileVisits)
df$counts_kissesStd <- scale(df$counts_kisses)
df$counts_picturesStd <- scale(df$counts_pictures)


```

# Part 2: Exploratory Data Analysis

The goal of this section is to uncover relationships within the dataset. Starting off, a visual analysis of the different variables is conducted to assess their potential relevance. This is followed by a stepwise regression to help identify the variables that might lead to the most accurate model. However, it's worth noting that the results of the stepwise regression should guide, but not entirely determine, the final model selection. Certain variables might not pass the 'stepwise' threshold, yet they might still hold valuable insights.

As alluded to in the introduction, it becomes clear that certain variables influence the number of likes, while others predominantly affect profile views. This distinction is particularly important because some profile characteristics are only observable when a profile is actually being viewed - a profile biography, for example. Nonetheless, a substantial correlation between profile views and likes is evident. This relationship is visually represented in Figure 2 below.

#### Figure 2: Bubble plot of profile views and number of pictures in profile. A non-linear model (loess method) was fitted on the plot to discern possible patterns and differences between bios with emojis and those without. Size of dots present how detailed the account is.

```{r echo=FALSE, fig.align="center", message=FALSE}
df %>%
  filter(counts_profileVisits < 100000) %>% 
  ggplot(aes(x = counts_profileVisits, y = counts_kisses)) +
  geom_point(aes(fill = factor(has_emoji), size = counts_details), pch = 21, alpha = 0.8, colour = "white") +
  scale_fill_manual(values = c("#1beaa7", "#8c2aef"), name = "Has Emoji", labels = c("No Emoji", "Contains Emoji")) +
  scale_size(range = c(0.1, 4)) +
  labs(x = "Profile Visits Count", y = "Number of Kisses", size = "Details Count", fill = "Contains Emoji") +
  th +
  geom_smooth(aes(colour = factor(has_emoji)), method = "loess", se = FALSE) +
  scale_colour_manual(values = c("#1beaa7", "#8c2aef"), name = "Has Emoji", labels = c("No Emoji", "Contains Emoji"))

```

## Biography characteristics and popularity

Figure 3 below seeks to present whether there is a difference in the distribution of likes received based on the newly created dummy variables, `has_emoji`, `contains_popular_word`, and `night_owl`. There seem to be some slight differences in likes received, supporting the idea that the use of emojis and certain words do suggest higher levels of trust. Being online during night time also may increase profile views, but I view this variable more as a control variable, rather than a causal one, as more people tend to be online during night time than in day time.

#### Figure 3: Boxplots showing effects of profile characteristics on popularity. Left panel: effect of a bio containing social media particulars and/or an emoji on likes received. Right panel: effect of an online profile and/or being a night owl on number of profile visits.

```{r, echo=FALSE, fig.align="center", fig.height= 10, fig.width=7, warning=FALSE}
plot1 <- df %>%
  filter(counts_kissesStd < 2, !is.na(night_owl), !is.na(has_social)) %>%
  mutate(has_emoji = factor(has_emoji, labels = c("No", "Yes")),
         night_owl = factor(night_owl, labels = c("No", "Yes")),
         has_social = factor(has_social, labels = c("No", "Yes"))) %>%
  pivot_longer(cols = c(has_emoji, has_social), names_to = "Factor", values_to = "Value") %>%
  mutate(Factor = recode(Factor, 
                         has_emoji = "Emoji",
                         has_social = "Social Media")) %>%
  ggplot(aes(x = Value, y = counts_kissesStd, fill = Value)) +
  geom_boxplot(colour = "white", alpha = 0.8) +
  scale_fill_manual(values = c("#1beaa7", "#8c2aef")) +
  facet_wrap(~ Factor, scales = "free", strip.position = "top") +
  labs(x = "", y = "Profile Kisses Count", fill = "") +
  th +
  theme(
    strip.background = element_blank(),
    strip.text = element_text(face = "bold", size = 14),
    axis.text.x = element_blank(),
    legend.position = "none"
  ) +
  scale_y_continuous(breaks = c(-1, 0, 1, 2))

plot2 <- df %>%
  filter(counts_profileVisitsStd < 2, !is.na(night_owl), !is.na(isOnline)) %>%
  mutate(night_owl = factor(night_owl, labels = c("No", "Yes")),
         isOnline = factor(isOnline, labels = c("No", "Yes"))) %>%
  pivot_longer(cols = c(night_owl, isOnline), names_to = "Factor", values_to = "Value") %>%
  mutate(Factor = recode(Factor,
                         night_owl = "Night Owl",
                         isOnline = "Online")) %>%
  ggplot(aes(x = Value, y = counts_profileVisitsStd, fill = Value)) +
  geom_boxplot(colour = "white", alpha = 0.8) +
  scale_fill_manual(values = c("#1beaa7", "#8c2aef")) +
  facet_wrap(~ Factor, scales = "free", strip.position = "top") +
  labs(x = "", y = "Profile Visits Count", fill = "") +
  th +
  theme(
    strip.background = element_blank(),
    strip.text = element_text(face = "bold", size = 14),
    axis.text.x = element_blank()
  ) +
  scale_y_continuous(breaks = c(-1, 0, 1, 2))
```

<div class="row">

::: col-md-6
```{r echo=FALSE}
plot1
```
:::

::: col-md-6
```{r echo=FALSE}
plot2
```
:::

## Geographical characteristics and popularity

By using the Google Maps API, I was able to geocode the locations of every profile in the data. The goal is to visualise how location effects profile views. The code chunk below was used to geocode the data.

```{r}
# Note: commented out, due to costs associated with geocoding through the API
# df_city <- df %>% 
#   select(c(city, country, counts_profileVisits)) %>% 
#   mutate(address = paste0(city, ", ", country)) %>%
#   group_by(address) %>%
#   summarise(mean_profile_views = mean(counts_profileVisits, na.rm = TRUE))
# 
# df_city <- df_city %>%
#   mutate(geocode_data = map(address, ~geocode(.x, source = "google", output = "latlon")),
#          lon = map_dbl(geocode_data, "lon"),
#          lat = map_dbl(geocode_data, "lat"))
# 
# write_csv(df_city, "geocode_latlon.csv")

```

#### Figure 4: Geographic data visualisation of profile views. Size and colour of bubbles in top panel indicate profile views. Colour of country in the bottom panel indicate profile views.

```{r warning=FALSE, fig.align='center',  fig.height= 8, fig.width=7.65, echo=FALSE}
df_city <- read_excel("geocode_latlon.xlsx")

df_city <- df_city %>%
  mutate(mean_profile_views = as.numeric(mean_profile_views),
         lat = as.numeric(lat),
         lon = as.numeric(lon)) %>% 
  filter(!is.na(lon),
         mean_profile_views > 0,
         lon > -90 & lon < 100,
         lat > 0)

# Filter by countries after data cleaning and transformation
df_city <- df_city %>%
  filter(substr(address, nchar(address) - 1, nchar(address)) %in% eu_countries$code | address == "UK"| address == "CZ" | address == "CH")

world <- map_data("world") %>%
  filter(region %in% eu_countries$name | region == "UK" | region == "Czech Republic" | region == "Switzerland")

# Creating ggplot with map

map_bg <- ggplot(data = world) +
  geom_polygon(aes(x = long, y = lat, group = group), fill = "white", colour = "#000123") +
  coord_map() +
  theme_void()

# Adding scatterplot on the map
views <- map_bg +
  geom_point(data = df_city, aes(x = lon, y = lat, color = mean_profile_views, size = mean_profile_views), alpha = 0.8) +
  scale_colour_gradient(
    name = 'Mean Profile Views',
    limits = range(df_city$mean_profile_views),
    low = "#8c2aef",
    high = "#1beaa7"
  ) +
  scale_size_continuous(guide = "none", range = c(1, 8)) +
  theme(plot.background = element_rect(fill = "#000123", color = "#000123"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text = element_blank(),
        legend.key = element_rect(fill = "#000123", color = "#000123"),
        legend.background = element_rect(fill = "#000123"),
        legend.text = element_text(colour = "white"),
        legend.title = element_text(colour = "white", face = "bold"))

print(views)


# Group by country and calculate mean profile views
df_city_grouped <- df_city %>%
  mutate(country_code = substr(address, nchar(address) - 1, nchar(address))) %>%
  group_by(country_code) %>%
  summarise(mean_profile_views = mean(mean_profile_views, na.rm = TRUE))

# Convert two-letter country codes to full country names
df_city_grouped <- left_join(df_city_grouped, eu_countries, by = c("country_code" = "code"))

# Join with the 'world' dataframe
world <- left_join(world, df_city_grouped, by = c("region" = "name")) 
world <- world %>% 
  filter(!is.na(mean_profile_views))

# Create the choropleth map
choropleth <- map_bg + 
  geom_polygon(data = world, aes(x = long, y = lat, fill = mean_profile_views, group = group), color = "#000123") +
  scale_fill_gradient(name = 'Mean Profile Views',
                      low = "#8c2aef",
                      high = "#1beaa7",
                      na.value = "") +
  theme(plot.background = element_rect(fill = "#000123", color = "#000123"),
        legend.key = element_rect(fill = "#000123", color = "#000123"),
        legend.background = element_rect(fill = "#000123"),
        legend.text = element_text(colour = "white"),
        legend.title = element_text(colour = "white", face = "bold"))

print(choropleth)


```

Geographically presenting the data shows that profiles from some countries do indeed tend to receive more attention. It is necessary, however, to understand whether there is a different factor carrying mean profile views for different countries. Below I have created lollipop charts that show number of users for each specific region. From figure 4 below, it is clear that the country an individual comes from may not be a good indicator of fame in the data to my disposal. This is because of sample size bias. As such, it is advised to omit this variable from the final model.

#### Figure 5: Lollipop chart of number of users by country. The colour of the lollipops indicate mean profile views.

```{r echo=FALSE, fig.align='center'}

# Count the number of users per country in the 'df' dataframe
df_country_users <- df %>%
  group_by(country) %>%
  summarise(num_users = n())

# Convert two-letter country codes to full country names in df_country_users
df_country_users <- left_join(df_country_users, eu_countries, by = c("country" = "code"))

# Summarise mean_profile_views per country in df_city dataframe
df_city_grouped <- df_city %>%
  mutate(country_code = substr(address, nchar(address) - 1, nchar(address))) %>%
  group_by(country_code) %>%
  summarise(mean_profile_views = mean(mean_profile_views, na.rm = TRUE))

# Join df_city_grouped with df_country_users to add the number of users per country
df_city_grouped <- left_join(df_city_grouped, df_country_users, by = c("country_code" = "country"))

# Create the lollipop chart with number of users and colored by mean profile views
ggplot(df_city_grouped, aes(x = reorder(name, -num_users), y = num_users)) +
  geom_segment(aes(xend = name, yend = 0), color = "white") +
  geom_point(aes(color = mean_profile_views), size = 3, alpha = 0.8) +
  coord_flip() +
  scale_color_gradient(name = 'Mean Profile Views', low = "#8c2aef", high = "#1beaa7") +
  xlab("") +
  ylab("Number of Users") + th +
  theme(legend.position = "right",
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())

```

## Other profile characteristics and popularity

The goal of this section is to understand whether certain details on a profile affect popularity on a dating app.

#### Figure 6: Correlogram of profile characteristics and number of likes received.

```{r warning=FALSE, fig.align='center',  fig.height= 6.81, fig.width=8, echo=FALSE}

# Calculate the correlation matrix
correlation_matrix <- cor(df[c("counts_kissesStd", "age", "lang_count", "flirtInterests_chat", "flirtInterests_friends", 
                               "flirtInterests_date", "isMobile", "verified", "shareProfileEnabled")])

# Melt the correlation matrix
correlation_matrix_melt <- melt(correlation_matrix)

# Create the ggplot object
ggplot(data = correlation_matrix_melt, aes(x=Var1, y=Var2, fill=value)) +
    geom_tile(color = "white") +
    geom_text(aes(Var1, Var2, label = round(value, 2)), color = "black", size = 4) +
    scale_fill_gradient2(low = "#8c2aef", high = "#1beaa7", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Correlation") +
    th + 
    theme(
      strip.background = element_blank(),
      strip.text = element_text(face = "bold", size = 14),
      axis.text.x = element_text(angle = 90, vjust = 1, 
                                 size = 12, hjust = 1),
      axis.text.y = element_text(size = 12),
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      legend.position = "right") +
    coord_fixed()

```
Language may also play a role in popularity. In figure 6 I accounted for this by acknowledging the number of languages spoken as a possible predictor of popularity. In figure 7 below I visualise the distribution of the number of likes received based on specific spoken languages.

####    Figure 7: Ridgeline plot of languages spoken and number of likes received. Dashed line shows overall mean profile likes.

```{r warning=FALSE, fig.align='center',  fig.height= 8, fig.width=8, echo=FALSE, message=FALSE}

df_long <- df %>%
  pivot_longer(cols = starts_with("lang_"), 
               names_to = "language", 
               values_to = "spoken") %>%
  filter(spoken == TRUE) %>%
  mutate(language = str_remove(language, "lang_"),
         language = recode(language,
                           "de" = "German",
                           "en" = "English",
                           "es" = "Spanish",
                           "fr" = "French",
                           "it" = "Italian",
                           "pt" = "Portuguese"))

df_long %>%
  filter(language != "count", counts_kisses < 1000) %>% 
  ggplot(aes(x = counts_kisses, y = language, fill = language)) +
  geom_density_ridges(scale = 3, rel_min_height = 0.01, colour = "white", alpha = 0.8) +
  scale_fill_manual(values = palette) +
  geom_vline(aes(xintercept = 157.0227), linetype = "dashed", colour = "white") +
  theme_ridges() +
  th +
  theme(legend.position = "none") +
  labs(x = "Profile Kisses Count", y = "Language") +
  annotate("text", x = 170, y = Inf, label = "Mean Kisses Received (~160)", vjust = 2, hjust = 0, size = 4, colour = "white")


```



####    Figure 8: Dotted line plot of the number of pictures in profile and likes received. The mean number of likes received by number of photos was used to plot this relationship. Lines split based on social media tag presence in profile.

```{r echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}

df_summary <- df %>%
  group_by(counts_pictures, has_social) %>%
  summarise(mean_kisses = mean(counts_kisses, na.rm = TRUE), 
            .groups = "drop")

df_summary %>%
  filter(counts_pictures < 22 & counts_pictures > 0) %>% 
  ggplot(aes(x = as.factor(counts_pictures), y = mean_kisses, colour = factor(has_social), group = (has_social))) +
  geom_line(size = 1) +
  geom_point() +
  scale_colour_manual(values = c("#1beaa7", "#8c2aef"), 
                    name = "Has Social Media", 
                    labels = c("No Social", "Contains Social")) +
  labs(x = "Pictures Count", 
       y = "Mean Profile Kisses Count", 
       colour = "Contains Social Media") +
  th +
  theme(legend.position = "right")

```

# Part 3: Modelling & Results

After data preparation, we will split our data into training and testing datasets.

```{r}

# Set seed for reproducibility
set.seed(123)

# Define training and testing sets for profile visits prediction
split_visits <- initial_split(df, prop = 0.7, strata = "Profile_Views")
training_visits <- training(split_visits)
testing_visits <- testing(split_visits)


```

Similarly, we create training and testing sets for profile likes prediction:

```{r}

# Define training and testing sets for likes prediction
split_kisses <- initial_split(df, prop = 0.7, strata = "Profile_Likes")
training_kisses <- training(split_kisses)
testing_kisses <- testing(split_kisses)

```

##    Decision Tree Model

Now, we can proceed with training our decision tree models:

```{r}

# Train decision tree model for profile visits
visits_tree <- rpart(formula = Profile_Views ~ isOnline + night_owl + age,
                     data = training_visits, method = "class")
summary(visits_tree)

# Train decision tree model for profile likes
kisses_tree <- rpart(formula = Profile_Likes ~ has_emoji + has_social + Profile_Views + counts_pictures + lang_count + flirtInterests_chat + flirtInterests_date + flirtInterests_friends + counts_details,
                     data = training_kisses, method = "class")
summary(kisses_tree)


```

After building the models, let's make predictions on the testing data and evaluate the performance of our models:

```{r}

# Predict on testing data
predict_visits <- predict(visits_tree, newdata = testing_visits, type = "class")
predict_kisses <- predict(kisses_tree, newdata = testing_kisses, type = "class")

# Confusion matrices for evaluation
confusionMatrix(predict_visits, testing_visits$Profile_Views)
confusionMatrix(predict_kisses, testing_kisses$Profile_Likes)


```

Finally, we visualize the decision trees for a better understanding of our models:

```{r}

# Visualize the decision trees
rpart.plot(visits_tree, extra = 1)
rpart.plot(kisses_tree, extra = 1)

```
##   Random Forest Model

```{r}
# Set seed for reproducibility
# Set seed for reproducibility
set.seed(123)

# Filter out NA values
training_visits <- training_visits %>% filter(!is.na(night_owl))

# Random Forest model for profile visits
visits_rf <- randomForest(formula = Profile_Views ~ isOnline + night_owl + age,
                          data = training_visits,
                          importance = TRUE, 
                          ntree = 500)

# View model summary
visits_rf

# Random Forest model for profile likes
kisses_rf <- randomForest(formula = Profile_Likes ~ has_emoji + has_social + Profile_Views + counts_pictures + lang_count + flirtInterests_chat + flirtInterests_date + flirtInterests_friends + counts_details,
                          data = training_kisses,
                          importance = TRUE, 
                          ntree = 500)

# View model summary
kisses_rf

# Predict on test data
visits_rf_pred <- predict(visits_rf, newdata = testing_visits)
kisses_rf_pred <- predict(kisses_rf, newdata = testing_kisses)

# Confusion matrices
confusionMatrix(visits_rf_pred, testing_visits$Profile_Views)
confusionMatrix(kisses_rf_pred, testing_kisses$Profile_Likes)

# Variable importance plot
varImpPlot(kisses_rf)

```
##    XGBoost Model
```{r}

library(xgboost)

# Convert data to matrix format which is required by xgboost
train_matrix_visits <- model.matrix(Profile_Views ~ ., data = training_visits)
test_matrix_visits <- model.matrix(Profile_Views ~ ., data = testing_visits)

# Define parameters 
params <- list(
  objective = "multi:softmax", # multiclass classification 
  num_class = 4, # number of levels in dependent variable
  eta = 0.01, 
  max_depth = 6, 
  eval_metric = "mlogloss" # loss function for multiclass classification
)

# Train the model
visits_xgb <- xgboost(
  data = train_matrix_visits,
  label = as.numeric(training_visits$Profile_Views), 
  params = params,
  nrounds = 100,
  print_every_n = 10
)

# Predict on test data
visits_xgb_pred <- predict(visits_xgb, newdata = test_matrix_visits)
visits_xgb_pred <- as.factor(visits_xgb_pred)

# Convert data to matrix format for profile likes
train_matrix_kisses <- model.matrix(Profile_Likes ~ ., data = training_kisses)
test_matrix_kisses <- model.matrix(Profile_Likes ~ ., data = testing_kisses)

# Train the model
kisses_xgb <- xgboost(
  data = train_matrix_kisses,
  label = as.numeric(training_kisses$Profile_Likes), 
  params = params,
  nrounds = 100,
  print_every_n = 10
)

# Predict on test data
kisses_xgb_pred <- predict(kisses_xgb, newdata = test_matrix_kisses)
kisses_xgb_pred <- as.factor(kisses_xgb_pred)

# Confusion matrices
confusionMatrix(visits_xgb_pred, testing_visits$Profile_Views)
confusionMatrix(kisses_xgb_pred, testing_kisses$Profile_Likes)


```


# Discussion & Conclusion

The confusion matrix provides an overview of the performance of your classification model, and the additional statistics give more detailed insight into the model's predictive power.

Let's interpret the statistics for each confusion matrix.

Confusion Matrix 1:

Accuracy: The overall accuracy of the model is 31.43%. This means that the model correctly predicted the class (Low, Low Mid, High Mid, High) for around 31.43% of the observations in the test set. Given that random guessing would result in 25% accuracy (for a 4-class problem), the model performs slightly better than random guessing.

Kappa: The Kappa statistic is 0.0854, indicating a poor agreement between the predictions and the actual classes, as it is close to 0.

Sensitivity (Class Low): Sensitivity, also known as True Positive Rate or Recall, measures the proportion of actual positives that are correctly identified. For class "Low", this is 71.57%, indicating that the model correctly identifies about 72% of the actual "Low" class instances.

Specificity (Class Low): Specificity, also known as True Negative Rate, measures the proportion of actual negatives that are correctly identified. For class "Low", this is 43.74%, which means that the model correctly identifies about 44% of the instances that are not in the "Low" class.

The confusion matrix itself shows the counts of true and false positives and negatives. For example, the model predicted "Low" 717 times, but it was correct only 214 times. The model never predicted "Low Mid" or "High Mid".

Confusion Matrix 2:

Accuracy: The model's overall accuracy is much better in this case, at 71.92%. This is a significant improvement over the first model.

Kappa: The Kappa statistic is 0.6256, indicating a moderate to substantial agreement between predictions and actual classes.

Sensitivity (Class Low): For class "Low", the sensitivity is 80.66%, indicating that the model correctly identifies about 81% of the actual "Low" class instances.

Specificity (Class Low): For class "Low", the specificity is 94.03%, indicating that the model correctly identifies about 94% of the instances that are not in the "Low" class.

The confusion matrix again shows the counts of true and false positives and negatives. For example, the model predicted "Low" 299 times, and it was correct 246 times. This model makes predictions for all classes, unlike the first model.

The other statistics provide further insight into the model's performance, with measures for each class separately. Overall, the second model is performing much better than the first one.
